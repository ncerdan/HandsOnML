{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch_08_Dimensionality_Reduction.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSLxdgD07oDQCrJsmVWWlP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNLBB_N4l39E",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncerdan/HandsOnML/blob/master/Ch_08_Dimensionality_Reduction.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHVDkivdmX58",
        "colab_type": "text"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUk2uVGPmaZZ",
        "colab_type": "text"
      },
      "source": [
        "## Principal Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkUgWbWtm3Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a 3D dataset\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05aap6CBlwFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can compute them by hand using svd\n",
        "# if computing by hand, be sure to center the dataset!\n",
        "\n",
        "# compute svd\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, vT = np.linalg.svd(X_centered)\n",
        "\n",
        "# extract the first two PCs\n",
        "c1 = vT.T[:, 0]\n",
        "c2 = vT.T[:, 1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuLPjM3DnbbK",
        "colab_type": "text"
      },
      "source": [
        "## Projecting Down into d Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQTBeFQSnUgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to project into d-dimensions, just use the first d PCs\n",
        "# here: w=2\n",
        "\n",
        "W_2 = vT.T[:, :2]\n",
        "X_2D = X_centered.dot(W_2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2YvV1nJoWbG",
        "colab_type": "text"
      },
      "source": [
        "## Using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHrq2oaEoTY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "485eb019-6052-44f7-bb29-dda1c6b02875"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)   # projects to 2-D\n",
        "X_2D = pca.fit_transform(X)\n",
        "\n",
        "# the components_ attr holds the W_d matrix used\n",
        "print('Components:', pca.components_)\n",
        "\n",
        "# this would be the first PC\n",
        "print('First PC:', pca.components_.T[:, 0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Components: [[-0.93636116 -0.29854881 -0.18465208]\n",
            " [ 0.34027485 -0.90119108 -0.2684542 ]]\n",
            "First PC: [-0.93636116 -0.29854881 -0.18465208]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x0yFRfupOMp",
        "colab_type": "text"
      },
      "source": [
        "## Explained Variance Ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di2KhNNvpGFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16292726-132e-46be-9e7b-687628124240"
      },
      "source": [
        "# shows how much variance each PC contains\n",
        "pca.explained_variance_ratio_   # as you can see, the third PC only would have 2%"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84248607, 0.14631839])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wHG8Awp0ty",
        "colab_type": "text"
      },
      "source": [
        "## Choosing the Right Number of Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo4wfmw2qn6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first lets load in MNIST\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "mnist_target = mnist.target.astype(np.uint8)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8FRdLUlrDN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and partition it\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = mnist['data']\n",
        "y = mnist['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuQ5rYmLprIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28ea1661-4e51-4aeb-cc9b-e91d45e019fe"
      },
      "source": [
        "# here, we can use pca without reducing the data then pick the smallest\n",
        "# dimension that maintains at least 95% of the variance\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumulativeSum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumulativeSum >= 0.95) + 1\n",
        "print(d)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sF20pSErlIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then we could run\n",
        "pca = PCA(n_components=d)\n",
        "X_reduced = pca.fit_transform(X_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRShbiWxsEUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# however, instead of this, this is built in to the PCA class\n",
        "# if you pass a float between 0 and 1 it will assume you are \n",
        "# setting the minimum variance retained:\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8cetrowsn-y",
        "colab_type": "text"
      },
      "source": [
        "## PCA for Compression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwgzk1L3sad5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this code 'compresses' the data, but then also reconstructs it\n",
        "pca = PCA(n_components=154)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxM4cYEmD-o6",
        "colab_type": "text"
      },
      "source": [
        "## Randomized PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbsmutrfD0B-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the svd_solver='randomized' uses a stochastic PCA algo\n",
        "#   the default value is 'auto' where sklearn uses the following values:\n",
        "#       'randomized': if m or n is greater than 500 and d is less than 80% of m or n\n",
        "#       'full' approach: otherwise\n",
        "\n",
        "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
        "X_reduced = rnd_pca.fit_transform(X_train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUqujZbFE1yk",
        "colab_type": "text"
      },
      "source": [
        "## Incremental PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0NxKqDJEei7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "# does mini-batches of size 100\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X_train, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced = inc_pca.transform(X_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIJkOGOiFlFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you could also use numpy's memmap class which allows you to operate on binary data \n",
        "# on disk as if it is all in memory\n",
        "\n",
        "# X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m, n))\n",
        "\n",
        "# batch_size = m // n_batches\n",
        "# inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
        "# inc_pca.fit(X_mm)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv4LjXtRGFju",
        "colab_type": "text"
      },
      "source": [
        "# Kernal PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPbgYCTmROvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get some dummy data\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "y = t > 6.9"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTTEFMYGDEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# would use sklearn's KernelPCA class imported as follows\n",
        "from sklearn.decomposition import KernelPCA"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIrprFnAHW9I",
        "colab_type": "text"
      },
      "source": [
        "## Selecting a Kernel and Tuning Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEn0R2tfHVvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "6c31e6a9-e9cc-42c4-b455-8b964c2d1fcf"
      },
      "source": [
        "# creates a simple 2-step pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('kpca', KernelPCA(n_components=2)),\n",
        "    ('log_reg', LogisticRegression())\n",
        "])\n",
        "\n",
        "param_grid = [{\n",
        "    'kpca__gamma': np.linspace(0.03, 0.05, 5),\n",
        "    'kpca__kernel': ['rbf', 'sigmoid']\n",
        "}]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('kpca',\n",
              "                                        KernelPCA(alpha=1.0, coef0=1,\n",
              "                                                  copy_X=True, degree=3,\n",
              "                                                  eigen_solver='auto',\n",
              "                                                  fit_inverse_transform=False,\n",
              "                                                  gamma=None, kernel='linear',\n",
              "                                                  kernel_params=None,\n",
              "                                                  max_iter=None, n_components=2,\n",
              "                                                  n_jobs=None,\n",
              "                                                  random_state=None,\n",
              "                                                  remove_zero_eig=False,\n",
              "                                                  tol=0)),\n",
              "                                       ('log_reg',\n",
              "                                        LogisticRegression(C=1.0,\n",
              "                                                           cl...\n",
              "                                                           max_iter=100,\n",
              "                                                           multi_class='auto',\n",
              "                                                           n_jobs=None,\n",
              "                                                           penalty='l2',\n",
              "                                                           random_state=None,\n",
              "                                                           solver='lbfgs',\n",
              "                                                           tol=0.0001,\n",
              "                                                           verbose=0,\n",
              "                                                           warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'kpca__gamma': array([0.03 , 0.035, 0.04 , 0.045, 0.05 ]),\n",
              "                          'kpca__kernel': ['rbf', 'sigmoid']}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gfjQau6K-c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# can also create the pre-image\n",
        "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433, fit_inverse_transform=True) # last param allows for inverse_transform\n",
        "\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuxtX_daQ7Ds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ce337a2-a9a9-4f55-aa80-7c232d700054"
      },
      "source": [
        "# then you can compute the pre-image error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(X, X_preimage)       # would use this to gridsearch"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.78630879576616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVRR1KyS-bO",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQHfk9tAWqgw",
        "colab_type": "text"
      },
      "source": [
        "## 9.) Load MNIST, and compare training time before and after dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AqquGxjS7uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the data\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist['data']\n",
        "y = mnist['target']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev8fEBJpW8fE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into training and testing\n",
        "X_train = X[:60000]\n",
        "X_test = X[60000:]\n",
        "y_train = y[:60000]\n",
        "y_test = y[60000:]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5x0XCg9XQwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_fst_clf = RandomForestClassifier(n_estimators=50)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOKho5shX78U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "281955c9-f121-47b8-c74c-a21d6cb877f7"
      },
      "source": [
        "# time its training\n",
        "import time\n",
        "\n",
        "start_t = time.time()\n",
        "rnd_fst_clf.fit(X_train, y_train)\n",
        "end_t = time.time()\n",
        "\n",
        "print('time: {:.2f}s'.format(end_t - start_t))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 24.96s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_xLw3T9YQZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the reduced dataset\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_reduced = pca.fit_transform(X_train)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBQrH7YSYm_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d68f2a4d-d952-4af5-ed44-aafd88c241a2"
      },
      "source": [
        "# time the reduced training\n",
        "\n",
        "rnd_fst_clf2 = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "start_t_r = time.time()\n",
        "rnd_fst_clf2.fit(X_train_reduced, y_train)\n",
        "end_t_r = time.time()\n",
        "\n",
        "print('time: {:.2f}s'.format(end_t_r - start_t_r))  # on different classifier it would go faster --> not on random forests apparently"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 59.14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fOnKRALYvzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "84a34c0c-a841-4ba0-e7d1-72ba188c1fdf"
      },
      "source": [
        "# now lets check out their performances\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "\n",
        "y_pred_normal = rnd_fst_clf.predict(X_test)\n",
        "y_pred_redux  = rnd_fst_clf2.predict(X_test_reduced)\n",
        "\n",
        "print('Normal: ', accuracy_score(y_test, y_pred_normal))\n",
        "print('Reduced: ', accuracy_score(y_test, y_pred_redux))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal:  0.9669\n",
            "Reduced:  0.9457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6s6QgkUZwQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as you can see, reducing the dimension slightly lowered the performance (as expected)"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}